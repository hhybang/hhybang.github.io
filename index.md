---
layout: page
title: ""
class: home
---

# **Hello, I'm Hyemin (Helen) Bang**
### I'm an MEng student at MIT studying AI + HCI.
<!-- ### This website is under construction.  -->

<div class="columns" markdown="1">

<div class="intro" markdown="1">
<p>Currently, I work with <a href="https://arvindsatya.com/">Arvind Satyanarayan</a> in the <a href="https://vis.csail.mit.edu/">MIT CSAIL Visualization Group</a>. My research focuses on human-centered AI, studying how to develop interpretable AI systems that align with human reasoning and empower users to make confident, informed decisions. </p>
<br >
<p>Previously, I earned a Bachelor of Science in Computer Science and Engineering from <a href="https://mit.edu/">MIT</a>. After graduating, I worked as a Systems Developer at <a href="https://intersystems.com/">InterSystems</a>, where I integrated machine learning capabilities into large-scale database platforms. </p>
<br >
<p>I am now applying to PhD programs to further study AI + HCI.</p>
</div>

<div class="me" markdown="1">

<picture>
  <source srcset='/images/hmbang.webp' type='image/webp' />
  <img
    src='/images/hmbang.jpg'
    alt='Hyemin Bang'>
</picture>


<div style="display: flex; justify-content: center;">
  <div class="link-buttons">
    <a class="button" href="https://drive.google.com/file/d/10CfkCFo01MXuVk6zRN9A-EmL7QRCjUII/view?usp=sharing">
      <div><b>CV</b></div>
    </a>
<!--     <a class="button" href="https://scholar.google.com/citations?user=pQd1HSK5lzEC">
      <div><i class="fa-solid fa-graduation-cap"></i></div>
    </a> -->
    <!-- <a class="button" href="https://github.com/hhybang">
      <div><i class="fa-brands fa-github"></i></div>
    </a> -->
    <a class="button" href="https://www.linkedin.com/in/hyeminbang/">
      <div><i class="fa-brands fa-linkedin-in"></i></div>
    </a>
    <a class="button" href="mailto:{{ site.email }}">
      <div><i class="fa-solid fa-envelope"></i></div>
    </a>
  </div>
</div>

<!-- <a href="mailto:{{ site.email }}">{{ site.email }}</a> -->
</div>
</div>


## Featured Projects
<div class="featured-projects">
    <div class="project">
      <div class="preview-image" style="background-image: url('/images/projects/explanation_alignment.png');"></div>
      <div class="project-content">
        <div class="title"><a href="https://vis.csail.mit.edu/pubs/explanation-alignment/">Explanation Alignment: Quantifying the Correctness of Model Reasoning At Scale</a></div>
        <p>Explanation alignment measures the agreement between model-generated explanations and human annotations to distinguish models relying on spurious correlations and model biases by aggregating saliency-based metrics, such as Shared Interest and The Pointing Game, across datasets.</p>
        <div class="links">
            <a href="https://github.com/mitvis/explanation_alignment"><i class="fa-brands fa-github" aria-hidden="true"></i> Code</a>
            <a href="https://vis.csail.mit.edu/pubs/explanation-alignment.pdf"><i class="fa-file-pdf far" aria-hidden="true"></i> Paper</a>
        </div>
      </div>
    </div>
</div>

## Teaching
<div class="featured-projects">
    <div class="project">
      <div class="preview-image" style="background-image: url('/images/projects/explanation_alignment.png');"></div>
      <div class="project-content">
        <div class="title"><a href="https://vis.csail.mit.edu/pubs/explanation-alignment/">Explanation Alignment: Quantifying the Correctness of Model Reasoning At Scale</a></div>
        <p>Explanation alignment measures the agreement between model-generated explanations and human annotations to distinguish models relying on spurious correlations and model biases by aggregating saliency-based metrics, such as Shared Interest and The Pointing Game, across datasets.</p>
        <div class="links">
            <a href="https://github.com/mitvis/explanation_alignment"><i class="fa-brands fa-github" aria-hidden="true"></i> Code</a>
            <a href="https://vis.csail.mit.edu/pubs/explanation-alignment.pdf"><i class="fa-file-pdf far" aria-hidden="true"></i> Paper</a>
        </div>
      </div>
    </div>
    <div class="project">
      <div class="preview-image" style="background-image: url('/images/034_photo.png');"></div>
      <div class="project-content">
        <div class="title"><a href="https://vis.csail.mit.edu/pubs/explanation-alignment/">Explanation Alignment: Quantifying the Correctness of Model Reasoning At Scale</a></div>
        <p>Explanation alignment measures the agreement between model-generated explanations and human annotations to distinguish models relying on spurious correlations and model biases by aggregating saliency-based metrics, such as Shared Interest and The Pointing Game, across datasets.</p>
        <div class="links">
            <a href="https://github.com/mitvis/explanation_alignment"><i class="fa-brands fa-github" aria-hidden="true"></i> Code</a>
            <a href="https://vis.csail.mit.edu/pubs/explanation-alignment.pdf"><i class="fa-file-pdf far" aria-hidden="true"></i> Paper</a>
        </div>
      </div>
    </div>
    <div class="project">
      <div class="preview-image" style="background-image: url('/images/projects/explanation_alignment.png');"></div>
      <div class="project-content">
        <div class="title"><a href="https://vis.csail.mit.edu/pubs/explanation-alignment/">Explanation Alignment: Quantifying the Correctness of Model Reasoning At Scale</a></div>
        <p>Explanation alignment measures the agreement between model-generated explanations and human annotations to distinguish models relying on spurious correlations and model biases by aggregating saliency-based metrics, such as Shared Interest and The Pointing Game, across datasets.</p>
        <div class="links">
            <a href="https://github.com/mitvis/explanation_alignment"><i class="fa-brands fa-github" aria-hidden="true"></i> Code</a>
            <a href="https://vis.csail.mit.edu/pubs/explanation-alignment.pdf"><i class="fa-file-pdf far" aria-hidden="true"></i> Paper</a>
        </div>
      </div>
    </div>
</div>
